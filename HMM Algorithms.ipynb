{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we wrote several HMM algorithms for NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words and POS Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the Brown Corpus downloaded by NLTK. The corpus has provided POS tags for the words and punctuation, so that we could use them to train our HMM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 500\n",
      "Number of words: 1161192\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "fileids = brown.fileids()\n",
    "tagged_words = brown.tagged_words()\n",
    "print 'Number of files:', len(fileids)\n",
    "print 'Number of words:', len(tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 500 files in the corpus, in order to validate our model, we need to split the data into training and testing parts. We randomly selected 400 articles as training part and the rest as testing part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(100)\n",
    "fileids_testing = random.sample(fileids, 80)\n",
    "fileids_training = fileids\n",
    "for item in fileids_testing:\n",
    "    fileids_training.remove(item) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training words: 787989\n",
      "Length of testing words: 186742\n"
     ]
    }
   ],
   "source": [
    "tagged_words_training = brown.tagged_words(fileids_training)\n",
    "tagged_words_testing = brown.tagged_words(fileids_testing)\n",
    "print 'Length of training words:', len(tagged_words_training)\n",
    "print 'Length of testing words:', len(tagged_words_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "words_training = [item[0] for item in tagged_words_training]\n",
    "tags_training = [item[1] for item in tagged_words_training]\n",
    "words_testing = [item[0] for item in tagged_words_testing]\n",
    "tags_testing = [item[1] for item in tagged_words_testing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of different words 45060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'the', 42810),\n",
       " (u',', 39463),\n",
       " (u'.', 33499),\n",
       " (u'of', 24710),\n",
       " (u'and', 18907),\n",
       " (u'to', 17613),\n",
       " (u'a', 14796),\n",
       " (u'in', 13307),\n",
       " (u'that', 7109),\n",
       " (u'is', 7022),\n",
       " (u'was', 6564),\n",
       " (u'for', 5990),\n",
       " (u'``', 5751),\n",
       " (u\"''\", 5743),\n",
       " (u'The', 4961),\n",
       " (u'with', 4741),\n",
       " (u'it', 4595),\n",
       " (u'as', 4520),\n",
       " (u'be', 4477),\n",
       " (u'on', 4288)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_training_count = Counter(words_training)\n",
    "print 'Number of different words', len(words_training_count.keys())\n",
    "words_training_count.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undoubtedly, most frequent words are stop words and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different tags 421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'NN', 104440),\n",
       " (u'IN', 82070),\n",
       " (u'AT', 66785),\n",
       " (u'JJ', 44002),\n",
       " (u'.', 40966),\n",
       " (u',', 39323),\n",
       " (u'NNS', 37804),\n",
       " (u'CC', 25524),\n",
       " (u'RB', 24779),\n",
       " (u'NP', 23128),\n",
       " (u'VB', 23024),\n",
       " (u'VBN', 19809),\n",
       " (u'VBD', 17203),\n",
       " (u'CS', 15073),\n",
       " (u'VBG', 12086),\n",
       " (u'PPS', 11966),\n",
       " (u'PP$', 11081),\n",
       " (u'TO', 10301),\n",
       " (u'PPSS', 9351),\n",
       " (u'CD', 9197)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_training_count = Counter(tags_training)\n",
    "print 'Number of different tags', len(tags_training_count.keys())\n",
    "tags_training_count.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 421 different tags. We need to check whether all the tags of testing data belong to those of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different tags 291\n",
      "key PPSS+BER-TL is not in training tags.\n",
      "key EX-HL is not in training tags.\n",
      "key CC-TL-HL is not in training tags.\n",
      "key HV-HL is not in training tags.\n",
      "key JJ$-TL is not in training tags.\n",
      "key FW-*-TL is not in training tags.\n",
      "key ,-TL is not in training tags.\n",
      "key NR-TL-HL is not in training tags.\n",
      "key *-TL is not in training tags.\n",
      "key FW-PP$ is not in training tags.\n",
      "key FW-BE is not in training tags.\n",
      "key FW-CD-TL is not in training tags.\n",
      "key WRB+IN is not in training tags.\n",
      "key WRB+DOZ is not in training tags.\n",
      "key HV-TL is not in training tags.\n",
      "key FW-RB-TL is not in training tags.\n",
      "key WRB+DO is not in training tags.\n",
      "key HV+TO is not in training tags.\n",
      "key FW-VBD is not in training tags.\n",
      "key FW-JJT is not in training tags.\n",
      "key FW-QL is not in training tags.\n",
      "key JJ-TL-NC is not in training tags.\n",
      "key FW-DT+BEZ is not in training tags.\n",
      "key FW-AT-HL is not in training tags.\n",
      "key PN+HVD is not in training tags.\n",
      "key BEN-TL is not in training tags.\n"
     ]
    }
   ],
   "source": [
    "tags_testing_count = Counter(tags_testing)\n",
    "print 'Number of different tags', len(tags_testing_count.keys())\n",
    "new_tags = []\n",
    "for key in tags_testing_count.keys():\n",
    "    if key not in tags_training_count.keys():\n",
    "        new_tags.append(key)\n",
    "        print 'key', key, 'is not in training tags.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many new tags in the testing data. We could remove them so that all the tags of testing data would be within those of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Testing words: 186705\n"
     ]
    }
   ],
   "source": [
    "for word, tag in tagged_words_testing:\n",
    "    if tag in new_tags:\n",
    "        words_testing.remove(word)\n",
    "        tags_testing.remove(tag)\n",
    "\n",
    "print 'Length of Testing words:', len(words_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems only 37 words and tags have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_testing_count = Counter(words_testing)\n",
    "tags_testing_count = Counter(tags_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We considered **bigram model** here. We define c(u, s) to be the number of times the sequence of two states (u, s) is seen in training data.\n",
    "Define c(s) to be the number of times that the state s is seen in the corpus. Finally, define c(s - x) to be the number of times state s is seen paired sith observation x in the corpus: for example, c(N - dog) would be the number of times the word dog is seen paired\n",
    "with the tag N.\n",
    "\n",
    "*q(s|u) = c(u, s)/c(u)*\n",
    "\n",
    "*e(x|s) = c(s - x)/c(s)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need calculate the transition matrix *q* and emission matrix *e* based on the corpus mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observations = words_training_count.keys()#Observations are words\n",
    "states = tags_training_count.keys()#states are the tags\n",
    "states_len = len(states)\n",
    "obs_len = len(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we calculated the frequencies of each tag and bigram tags in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unigram_freq = {}\n",
    "for st in states:\n",
    "    unigram_freq[st] = tags_training_count[st]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_freq = dict()\n",
    "for i in range(len(tags_training) - 1):\n",
    "    u = tags_training[i]\n",
    "    s = tags_training[i+1]\n",
    "    bigram = (u, s)\n",
    "    bigram_freq[bigram] = bigram_freq.get(bigram, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we could calculate the probabilities of each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_p = {}\n",
    "for st in states:\n",
    "    unigram_p[st] = float(tags_training_count[st])/states_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, calculate the probabilities of bigram tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_p = {}\n",
    "for k, v in bigram_freq.items():\n",
    "    #print k, v\n",
    "    if unigram_freq[k[0]] != 0:\n",
    "        bigram_p[k] = float(v)/unigram_freq[k[0]]#p(yi|yi-1)\n",
    "    else:\n",
    "        bigram_p[k] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([ v == 0 for v in bigram_p.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, there are no zero bigrams. Next, we calculated the condition probabilities between the tags and the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53233"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_word_freq = Counter(tagged_words_training)\n",
    "len(tag_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_word_p = {}\n",
    "for k, v in tag_word_freq.items():\n",
    "    if unigram_freq[k[1]] != 0:\n",
    "        tag_word_p[k] = float(v)/unigram_freq[k[1]]#p(xi|yi)\n",
    "    else:\n",
    "        tag_word_p[k] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform the dictionaries into matrix for latter algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "trans_p = np.zeros([states_len, states_len])#there are 421 unique tags\n",
    "emit_p = np.zeros([obs_len, states_len])#there are 45060 unique words and punctuation\n",
    "start_p = np.zeros(states_len)#probabilities of each tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we could get the probabilities of each tag, namely the start probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for st in states:\n",
    "    start_p[i] = unigram_p[st]\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, calculated the transformation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for st in states:\n",
    "    j = 0\n",
    "    for st2 in states:\n",
    "        bigram = (st, st2)\n",
    "        if bigram in bigram_p.keys():\n",
    "            trans_p[i, j] = bigram_p[bigram]        \n",
    "        j = j + 1\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculated the emission matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-09ffdc12139c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mtag_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mtag_word\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtag_words_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m             \u001b[0memit_p\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtag_word_p\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtag_word\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mtag_words_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tag_words_list = tag_word_p.keys()\n",
    "for i in range(obs_len):\n",
    "    for j in range(states_len):\n",
    "        tag_word = (observations[i], states[j])\n",
    "        if tag_word in tag_words_list:\n",
    "            emit_p[i, j] = tag_word_p[tag_word]\n",
    "            tag_words_list.remove(tag_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53233"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tag_word_p.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "states = ('Healthy', 'Fever')\n",
    "observations = ('normal', 'cold', 'dizzy')\n",
    "start_p = np.array([0.6, 0.4])\n",
    "stop_p = np.array([1, 1])\n",
    "trans_p = np.array([[0.7, 0.4], [0.3, 0.6]])\n",
    "emit_p = np.array([[0.5, 0.1], [0.4, 0.3], [0.1, 0.6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Viteri Algorithm for Bigram HMM Model\n",
    "#start_p: probability of start state\n",
    "#trans_p: transition probabilities, matrix\n",
    "#emit_p: emission probabilities, matrix\n",
    "def BigramViterbi(start_p, trans_p, emit_p):\n",
    "    try:\n",
    "        y_len = trans_p.shape[0]\n",
    "        x_len = emit_p.shape[0]\n",
    "        pi = np.zeros([x_len+1, y_len+1])#maximum probability matrix for any sequence of length\n",
    "        bp = np.zeros([x_len+1, y_len+1])#backpoint matrix\n",
    "        #Base case\n",
    "        pi[0, 0] = 1\n",
    "        for i in range(1, y_len+1):\n",
    "            pi[0, i] = 0\n",
    "        #Find max probability\n",
    "        for i in range(1, x_len+1):\n",
    "            for j in range(1, y_len+1):\n",
    "                if i == 1:#Find the max porbability for pi(1, j)\n",
    "                    pi[i, j] = start_p[j-1] * emit_p[i-1, j-1]\n",
    "                else:\n",
    "                    for l in range(y_len):\n",
    "                        pi_new = pi[i-1, l+1] * trans_p[j-1, l] * emit_p[i-1, j-1]\n",
    "                        if pi[i, j] < pi_new:\n",
    "                            pi[i, j] = pi_new\n",
    "                            bp[i, j] = l + 1\n",
    "        return pi, bp\n",
    "    except:\n",
    "        print traceback.print_exc()  \n",
    "        return None\n",
    "\n",
    "def FindBestState(stop_p, pi, bp):\n",
    "    try:\n",
    "        x_len = pi.shape[0]\n",
    "        y_len = pi.shape[1]\n",
    "        max_prob = 0#max probability\n",
    "        max_st = 0#the index of state which has the max probability \n",
    "        state_list = []\n",
    "        #Find the max probability\n",
    "        for j in range(1, y_len):\n",
    "            prob = pi[x_len-1, j] * stop_p[j-1]\n",
    "            if prob > max_prob:\n",
    "                max_prob = prob\n",
    "                max_st = j \n",
    "        state_list.append(max_st)\n",
    "        #Find the max output states chain\n",
    "        pre_st = max_st\n",
    "        for i in range(x_len-1, 0, -1):\n",
    "            pre_st = bp[i, pre_st]\n",
    "            if i > 1: state_list.append(pre_st)\n",
    "        return max_prob, state_list\n",
    "    except:\n",
    "        print traceback.print_exc()  \n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.     ,  0.     ,  0.     ],\n",
       "        [ 0.     ,  0.3    ,  0.04   ],\n",
       "        [ 0.     ,  0.084  ,  0.027  ],\n",
       "        [ 0.     ,  0.00588,  0.01512]]), array([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  1.,  1.],\n",
       "        [ 0.,  1.,  1.]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BigramViterbi(start_p, trans_p, emit_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 1.0, 1.0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_len = pi.shape[1]\n",
    "print y_len\n",
    "FindBestState(stop_p, pi, bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Viterbi Algorithm for Trigram HMM Model\n",
    "#Input x_len: the length of the words.\n",
    "#Input y_len: the length of tags\n",
    "#Input q: the probability of seeing the tag s immediately after the bigram of tags (u, v)\n",
    "#Input e: the probability of seeing observation x paired with state s\n",
    "def TrigramViterbi(x_len, y_len, q, e):\n",
    "    #Check the input\n",
    "    if x_len == None or y_len == None or q == None or e == None:\n",
    "        return None\n",
    "    #Initialize the pi matrix\n",
    "    pi = np.zeros([x_len+1, y_len+1, y_len+1])\n",
    "    #Find the max probability for each ending tag(j, l) given k\n",
    "    for i in range(x_len+1):\n",
    "        for j in range(y_len+1):\n",
    "            for l in range(y_len+1):\n",
    "                #pi(0, *, *)=1\n",
    "                if i == 0 and j == 0 and l == 0:\n",
    "                    pi[i, j, l] = 1\n",
    "                if i == 1 and j == 0:\n",
    "                    pi[i, j, l] = q[l, 0, 0] * e[i, l]\n",
    "                if i == 2:\n",
    "                    pi[i, j, l] = pi[i-1, 0, j] * q[l, 0, j] * e[i, l]\n",
    "                #Find the max probability for each pi(i, j ,l)\n",
    "                if i > 2 and j > 0 and l > 0:\n",
    "                    for m in range(y_len):\n",
    "                        if pi[i, j, l]> pi[i-1, m+1, j] * q[l, m+1, j] *e[i, l]:\n",
    "                            pi[i, j, l] = pi[i-1, m+1, j] * q[l, m+1, j] *e[i, l]\n",
    "                        \n",
    "    return pi                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richard\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:8: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "pi = TrigramViterbi(x_len, y_len, q, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Viteri Algorithm for Bigram HMM Model\n",
    "#start_p: probability of start state\n",
    "#trans_p: transition probabilities, matrix\n",
    "#emit_p: emission probabilities, matrix\n",
    "def BigramViterbi(trans_p, emit_p):\n",
    "    y_len = trans_p.shape[0]\n",
    "    x_len = emit_p.shape[0]\n",
    "    pi = np.zeros([x_len+1, y_len+1])#maximum probability matrix for any sequence of length\n",
    "    bp = np.zeros([x_len+1, y_len+1])#backpoint matrix\n",
    "    #Base case\n",
    "    pi[0, 0] = 1\n",
    "    for i in range(1, y_len+1):\n",
    "        pi[0, i] = 0\n",
    "    #Find max probability\n",
    "    for i in range(1, x_len+1):\n",
    "        for j in range(1, y_len+1):\n",
    "            for l in range(y_len+1):\n",
    "                pi_new = pi[i-1, l] * trans_p[j-1, l] * emit_p[i-1, j-1]\n",
    "                if pi[i, j] < pi_new:\n",
    "                    pi[i, j] = pi_new\n",
    "                    bp[i, j] = l\n",
    "            print pi[i, j]\n",
    "    return pi, bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "states = ('Healthy', 'Fever')\n",
    "observations = ('normal', 'cold', 'dizzy')\n",
    "start_probability = np.array([0.6, 0.4)\n",
    "trans_p = np.array([[0.6, 0.7, 0.4], [0.4, 0.3, 0.6]])\n",
    "emit_p = np.array([[0.5, 0.1], [0.4, 0.3], [0.1, 0.6]])\n",
    "emit_p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
